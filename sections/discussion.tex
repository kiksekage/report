% !TeX root = ../report.tex

\section{Discussion}

\subsection{SemEval Task}

\subsubsection{Reusing tweets}
The full amount of unique tweets between task 1 and 5 is not easily inferable, but some rules can be deduced:\\
\begin{itemize}
\item All classification tweets are unique
\item All classification tweets have a regression label, but not vice versa
\item A tweet can appear in multiple regression emotions
\end{itemize}
Since the model is trained on the full 7102 regression tweets, two tweets with differing regressional values and feelings could map to the same classification labels. This can leave the model predicting ambiguously, since the same classification labels get associated with differing regression labels, words and character representations. This effect was not countered in any meaningful way, since it was not considered to be too disruptive. The effect could be countered by only selecting unique tweets, but this would then start interfering with the data given for the subtasks, and start losing relevance with regards to the tasks. 

\subsection{Deep learning}

\subsubsection{What should the model do and how should it do it?}
A choice was taken early in the process that the model should be as general as possible and take as much data with differing truth labels as possible. The model, as of now, takes the truth labels from task 1 and task 5. These two tasks were chosen because of their relatedness with regards to emotion inference from tweets, but difference in that they require regression and classification. The difference between task 1 and task 2 seem trivial, in that a mapping from a 0-to-1 regressional value can be mapped to an ordinal classification with relative ease, but the mapping from 0-to-1 regressional value to 11 binary flags, indicating emotions felt or not, seemed to require a more elaborate structure and loss function balancing.\\

VI SKAL SKRIVE MERE.\\
SKRIV NOGET OM AT VI BRUGER TASK 1 DATA SOM OM DET HELE VAR VALENS/AKTIVERING (ALTSÅ INGEN PRIMÆRE FØLELSER ASSOCIERET/INGEN SUBMODELS TRÆNET PR FØLELSE)

\subsubsection{Model iterations}
The first baby steps towards a multi task learning model was reached by solving the regression task first and having a secondary output which consisted of a 4 dimensional vector with sigmoid activation and a single output using a softmax layer which indicated which regression emotion was felt. This classification was easily implemented since the truth labels were readily available because of the regression task needing 4 different emotion groupings. This way of classifying tweets could be reintroduced as to avoid the ambiguity of the regression value output in the models current state.\\
FLERE ITERATIONER

\subsubsection{Loss functions; how to model the classification}
In one of the first iterations of the model, the classification task loss functions would be applied to a 11 dimensional vector, the output of the former model, to check whether or not flags were set. This loss function structure resulted in very conservative guesses. The predictions would have very little variance, and all the guesses seemed to favour zeros instead of ones, which can be explained by looking at the data presented in table \ref{tab:skew}. This was mitigated by outputting each emotion through its own, 1 dimensional layer with separate weights for the last classification. \\
\begin{table}[h]
\scalebox{0.8}{\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|}
& \text{Anger} & \text{Anticipation} & \text{Disgust} & \text{Fear} & \text{Joy} & \text{Love} & \text{Optimism} & \text{Pessimism} & \text{Sadness} & \text{Surprise} & \text{Trust} \\ \hline
\text{\# of flags set} & 2605 & 990 & 2666 & 1269 & 2499 & 699 & 2007 & 812 & 2076 & 365 & 352 \\
\text{\% of tweets} & 38\% & 14\% & 39\% & 19\% & 37\% & 10\% & 29\% & 12\% & 30\% & 5\% & 5\%
\end{tabular}}
\caption{The actual flags set and percentage of the full classification dataset}
\label{tab:skew}
\end{table}\\

As can be seen in table \ref{tab:skew}, flags are skewed towards certain emotions. This uneven distribution might have something to do with the ease of labelling a tweet with the emotions. Since the train, dev and test data for the task has been manually labelled, certain "human" effects might be felt and can be backpropagated through to the models built on the data. For instance, it might be very easy to infer anger in a tweet; an exclamation mark, a curse word or something similar might be used to infer the feeling, but how can you infer surprise? Or trust? Furthermore, 274 of the tweets had no feeling flags set, which was to be understood as a "neutral" feeling.\\
\\
The following is the main metric used for classification in the official SemEval task:\\
\begin{equation} \label{eq:accuracy}
Accuracy = \dfrac{1}{\lvert T \rvert} \sum_{t\in T}\dfrac{\lvert G_{t} \cap P_{t}\rvert}{\lvert G_{t} \cup P_{t}\rvert}
\end{equation}\\
LAV MÅSKE EN FODNOTE TIL EQUATION???\\
where $G_{t}$ is the set of the gold labels for tweet t, $P_{t}$ is the set of the predicted labels for tweet t, and T is the set of tweets. This metric can be problematic since it completely disregards tweets with zero flags set, which otherwise could present a nice and diverse understanding of feelings and their interplay on how people will write their tweets. The secondary official metrics are also problematic in that they also disregard "neutral" tweets. The secondary metrics are a combination of micro and macro precision, recall and f-scores. To mitigate this, the evaluation script used for local evaluation included a twelfth dimension which was only set if all the others were not set. This allowed for a more nuanced picture in the edge case were all of the flags were not set.