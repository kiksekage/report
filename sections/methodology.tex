% !TeX root = ../report.tex

\section{Methodology and theory}

\subsection{Residual networks}

\subsection{Recurrent networks and gated recurrent networks}
Because of the nature of tweets and the task at hand, a model was needed that could handle intricate features and their dependencies on each other through the full length of the tweets. Whether or not a sentence would show a strong emotional tendency towards fear or joy could sometimes rely on a negation at the start of the tweet, such as  [INSERT EXAMPLE HERE], for example. To accommodate this, a version of a recurrent neural network (RNN) was used, called gated recurrent units (GRUs).\\
\\
The GRU, as used in the model, are based on the work by [Cho, 2014 (ogs√• i Chung 2015)]. The activation of the GRU at a given time \textit{t} is the sum of the previous activation, $h_{t-1}^{j}$ and the candidate activation, $\tilde{h_{t}^{j}}$.\\
The activation $h_{t}^{j}$ of the GRU at time \textit{t} is:\\

\begin{equation} \label{eq:activation}
h_{t}^{j} = \left(1 - z_{t}^{j}\right)h_{t-1}^{j}+z_{t}^{j}\tilde{h_{t}^{j}},
\end{equation}\\

where the update gate $z_{t}^{j}$ decides how much of the unit is updated in the activation. The update gate is computed\\

\begin{equation}\label{eq:update}
z_{t}^{j}=\sigma\left(W_{z}\mathrm{x}_{t}+U_{z}\mathrm{h}_{t-1}\right)^{j},
\end{equation}\\

where \textit{U} and \textit{W} are weight matrices. The candidate activation, which is similar to a simple recurrent unit, is calculated [Bahdanau et al., 2014]\\

\begin{equation} \label{eq:candidate}
\tilde{h_{t}^{j}} = tanh\left(W\mathrm{x}_{t}+U\left(\mathrm{r}_{t}\odot\mathrm{h}_{t-1}\right)\right)^{j},
\end{equation}\\

where \textit{W} is the weights and $r_{t}$ is a set of reset gates and $\odot$ is the element-wise multiplication. When the reset gates are off (closer to 0) it allows the previous calculated steps to have less impact on the calculation of the candidate activation. The reset gate is calculated much like the update gate:\\

\begin{equation}\label{eq:reset}
r_{t}^{j}=\sigma\left(W_{r}\mathrm{x}_{t}+U_{r}\mathrm{h}_{t-1}\right)^{j}.
\end{equation}\\

The update gate can choose which dimensions to copy or ignore, depending on the activation of a sigmoid function. The reset gate controls which part of the current state gets to be included in the computation of the future state.\\
With regards to text, these GRUs can begin to make assumptions on which parts of the corpus will affect other parts of the corpus and can do so whether or not two interacting parts of the corpus are close to each other or not.

\subsection{Textual representation as word embeddings}
For the neural network models to be able to process and work with text, a numerical representation of the individual words needs to used. Since language carries a large amount of meaning and information in a more or less obscure and contextual sense, it is obvious that these these representations need to be of a high dimensionality, so as to be able to convey enough meaningful information so as to solve the task at hand.\\
The idea behind the word embeddings and how they can be used can be inferred from their high dimensional representations, a toy example:\\
\\
\begin{equation}
\begin{aligned}
\vec{car} = [0,0,5,0,5]\\
\vec{ship} = [0,0,3,0,4]\\
\vec{shampoo} = [4,0,2,0,0]
\end{aligned}
\end{equation}\\
Since car and ship are both a means of transport, it would make sense for them to be grouped closer together than for example shampoo and car. One could even go further and say that shampoo and ship might be grouped ever so slightly closer together than car and shampoo since they both concern themselves with water, but these deeper connections need higher and higher dimensional representations as to become clear.\\
These word vectors can then be fed into a neural network, which can in turn tune the weights of these words through backpropagation so that the higher level features, in our case, guessing the emotion intensity felt by a tweeter, can be derived.

\subsection{Nearest Centroid}
Nearest centroid is a classification model that works by assigning a new data point/vector to the label whose centroid(mean) is nearest. The model fits by taking the feature matrix and the corresponding labels and calculates the centroid of each of the labels. It then predicts by classifying the new data point to the closest centroid.
\subsection{Perceptron}
\subsection{Random Forest}
