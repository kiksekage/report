% !TeX root = ../report.tex

\section{Future work}

\subsection{Ensemble model}
Since the two presented models solve the same task and have different strengths and weaknesses, it would make sense for the models to be joined in a way that could utilize this. One easy, albeit naive way, would be to take the weighted sum of the two predictions in the regression task and apply some heuristic to the classification labels. The ratio between the summation of the two regression predictions could either be given as a hyperparameter but could also be introduced as a variable to be tweaked on by the deep learning model, so as to maximize the results on the train data. The classification heuristic could rely on the value of the regression predictions and an extra output from the deep learning model which would flag which of the models had hit the correct label on the train data.\\
\\
A more integrated ensemble model could be to feed the tf $\cdot$ idf weighted input as a tertiary input to the deep learning model as presented in section \ref{sec:deep} and then have a layer which consisted of the entire feature based model as outlined in section \ref{sec:feature}, with the overall architecture learnt experimentally. A tertiary output would then resemble the output produced by the feature based models as of current writing. The weights of this ensemble models layer would have to be completely independent from the other parts of the model as to act as a separate model to be conjoined later on in the evaluation. This model could build on the strengths of some of the more humanly recognizable features, and these have shown to have a positive effect for the model presented in \cite{ims}.\\
\\
Another version of the model that would take into consideration each emotion in task 1 and the differing emotion intensities on each tweet would be a model consisting of 5 submodels. One of these submodels would be the classification of the tweets, much like it is modelled now, same loss functions and augmentation of data. Each of the remaining four submodels should be a model specializing in one of the four regression emotions, with augmented truth labels for tweets that have no truth labels in the corresponding emotion. This model would then output a prediction for each emotion and a classification list for each tweet. By doing so, and knowing which emotion a tweet and its gold label is representing, a full spectrum of emotion intensities and binary emotion labels could be generated, with weights specializing on each of the emotions. The multitask part of the model would then be a weight sharing scheme between the four regression sub models and a weighting of these to be used in the classification.

\subsection{Generating noisy data for training}
One way to make the model more robust towards new data would be to be able to generate labelled data easily. Since the task relies on human annotated data, a bottleneck in the amount of available train data is the time and effort it takes to generate. If data that was mined could be annotated with a certain degree of trust in the predicted scores, the sheer amount of data that could then be made available could increase the models predictive capability.\\
A simple, yet effective model could be built on top of the lexicons shown in \cite{wassa2017}. Here, single lexicons predictive abilities are discussed and shown to achieve average regression ratings around 0.5 Pearson score (better than the model presented in this report!). These could then be applied to a large set of mined tweets and a noisy label could be produced. This method would rely on a very large amount of data to have a proper pay off, and steps have to be taken to ensure that the model would rather have a tendency to predict with high ranges than with safer, low range predictions so that a model trained on these noisy labels would not underfit completely.

\subsection{Tackling all subtasks and languages}
All subtasks presented at SemEval 2018 Task 1 had three variations: in english, arabic and spanish. The difficulties of applying the models presented in this would vary. The deep learning models only real difficulty would be that a corpus of pretrained word embeddings the size of the one presented in \cite{godin} might not be readily available, but since the model can still function without, it is only a matter of how well it would fare. It is possible that the model used on spanish data might fare comparative to the english data, since they build on (roughly) the same language structure. If arabic has a drastically different structure than english, one that depended more on subtleties such as suffixes, conjugations and other features that required a very nuanced textual representation, very large amounts of pretrained embeddings or very large word embedding dimensions would be needed to train the model. These speculations in what language need what and how/why are, however, far beyond the scope of this report.
