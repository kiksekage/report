% !TeX root = ../report.tex

\section{Feature based approach}

For the feature based approach, a feature extraction is done with regards to the term frequency - inverse document frequency. This has been the golden standard [CITE, speach and language processing page 278] for feature extraction, and ensures that the features extracted have the highest descriptive effect with regards to the tweets in which the feature occurs versus the tweets in which the feature does not. These extracted features are then used for the training of one of two different models, depending on the subtask at hand.\\
Both models builds on the random forest learning method since its properties supports both regression and multi-label classification. \\
The two models share the same general feature extraction, so they can be made directly comparable if the extraction parameters are kept comparatively the same.
\subsection{Custom features}
Besides the Tf-idf feature extraction method, we have implemented our own custom feature extraction function. In this function we have different flags, corresponding to different features that could be of interest in these particular tweets. \\
These different custom features involves e.g. whether or not a hashtag is present or if the spelling error of the whole tweet is above some threshold set by us. All of these custom features are boolean values in the sense that they are either present or not. \\
These features are then appended to the feature matrix returned by the Tf-idf extraction function, so that they are taken into consideration when training the different models.
\subsection{Model overview}
\subsubsection{Nearest centroid}
\subsubsection{Perceptron}
\subsubsection{Random Forest Regressor}